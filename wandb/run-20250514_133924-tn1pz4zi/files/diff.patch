diff --git a/__pycache__/a2c_pendulum.cpython-310.pyc b/__pycache__/a2c_pendulum.cpython-310.pyc
index 596b14a..93c64c1 100644
Binary files a/__pycache__/a2c_pendulum.cpython-310.pyc and b/__pycache__/a2c_pendulum.cpython-310.pyc differ
diff --git a/a2c_pendulum.py b/a2c_pendulum.py
index 73d9556..de1f84d 100644
--- a/a2c_pendulum.py
+++ b/a2c_pendulum.py
@@ -19,6 +19,11 @@ import argparse
 import wandb
 from tqdm import tqdm
 from typing import Tuple
+import os
+import yaml
+import datetime
+import json
+import csv
 
 def initialize_uniformly(layer: nn.Linear, init_w: float = 3e-3):
     """Initialize the weights and bias in [-init_w, init_w]."""
@@ -109,7 +114,7 @@ class A2CAgent:
         seed (int): random seed
     """
 
-    def __init__(self, env: gym.Env, args=None):
+    def __init__(self, env: gym.Env, args=None, is_test=False):
         """Initialize."""
         self.env = env
         self.gamma = args.discount_factor
@@ -117,7 +122,7 @@ class A2CAgent:
         self.seed = args.seed
         self.actor_lr = args.actor_lr
         self.critic_lr = args.critic_lr
-        self.num_episodes = args.num_episodes
+        self.num_episodes = getattr(args, 'num_episodes', 1000)
         
         # 設置網絡結構參數（如果有提供）
         self.actor_hidden_dim1 = getattr(args, 'actor_hidden_dim1', 128)
@@ -125,6 +130,21 @@ class A2CAgent:
         self.critic_hidden_dim1 = getattr(args, 'critic_hidden_dim1', 128)
         self.critic_hidden_dim2 = getattr(args, 'critic_hidden_dim2', 64)
         
+        # 設置保存相關參數
+        self.save_per_epoch = getattr(args, 'save_per_epoch', 100)  # 每隔多少個epoch保存一次
+        self.result_dir = getattr(args, 'result_dir', 'result-a2c_pendulum')
+        
+        # 創建實驗資料夾 (僅在非測試模式下)
+        self.is_test_mode = is_test
+        if not self.is_test_mode:
+            timestr = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
+            self.exp_dir = os.path.join(self.result_dir, f"exp_{timestr}")
+            if not os.path.exists(self.exp_dir):
+                os.makedirs(self.exp_dir, exist_ok=True)
+            
+            # 保存配置
+            self.save_config(args)
+        
         # device: cpu / gpu
         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
         print(self.device)
@@ -232,8 +252,15 @@ class A2CAgent:
         """Train the agent."""
         self.is_test = False
         step_count = 0
+        best_score = -float('inf')
         
-        for ep in tqdm(range(1, self.num_episodes)): 
+        # 創建結果記錄文件
+        result_file = os.path.join(self.exp_dir, 'training_results.csv')
+        with open(result_file, 'w', newline='') as f:
+            writer = csv.writer(f)
+            writer.writerow(['Episode', 'Score', 'Actor_Loss', 'Critic_Loss'])
+        
+        for ep in tqdm(range(1, int(self.num_episodes) + 1)): 
             actor_losses, critic_losses, scores = [], [], []
             state, _ = self.env.reset(seed=self.seed)
             score = 0
@@ -260,35 +287,173 @@ class A2CAgent:
                 # if episode ends
                 if done:
                     scores.append(score)
+                    avg_actor_loss = np.mean(actor_losses)
+                    avg_critic_loss = np.mean(critic_losses)
                     print(f"Episode {ep}: Total Reward = {score}")
+                    
+                    # 記錄結果
+                    with open(result_file, 'a', newline='') as f:
+                        writer = csv.writer(f)
+                        writer.writerow([ep, score, avg_actor_loss, avg_critic_loss])
+                    
                     # W&B logging
                     wandb.log({
                         "episode": ep,
-                        "return": score
-                        })  
+                        "return": score,
+                        "avg_actor_loss": avg_actor_loss,
+                        "avg_critic_loss": avg_critic_loss
+                        })
+                    
+                    # 保存檢查點
+                    if ep % self.save_per_epoch == 0:
+                        self.save_checkpoint(ep)
+                    
+                    # 保存最佳模型
+                    if score > best_score and not self.is_test_mode:
+                        best_score = score
+                        checkpoint = {
+                            'actor_state_dict': self.actor.state_dict(),
+                            'critic_state_dict': self.critic.state_dict(),
+                            'actor_optimizer': self.actor_optimizer.state_dict(),
+                            'critic_optimizer': self.critic_optimizer.state_dict(),
+                            'episode': ep,
+                            'score': best_score
+                        }
+                        torch.save(checkpoint, os.path.join(self.exp_dir, 'best_model.pt'))
+                        print(f"Saved best model with score {best_score} at episode {ep}")
+        
+        # 訓練結束後保存最終模型
+        if not self.is_test_mode:
+            self.save_checkpoint(int(self.num_episodes))
+
+    def load_checkpoint(self, checkpoint_path):
+        """加載模型檢查點"""
+        if not os.path.exists(checkpoint_path):
+            raise ValueError(f"Checkpoint file not found: {checkpoint_path}")
+        
+        checkpoint = torch.load(checkpoint_path, map_location=self.device)
+        
+        # 加載模型參數
+        self.actor.load_state_dict(checkpoint['actor_state_dict'])
+        self.critic.load_state_dict(checkpoint['critic_state_dict'])
+        
+        # 加載優化器參數（可選）
+        if 'actor_optimizer' in checkpoint and 'critic_optimizer' in checkpoint:
+            self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
+            self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
+            
+        episode = checkpoint.get('episode', 0)
+        score = checkpoint.get('score', None)
+        
+        print(f"Loaded checkpoint from episode {episode}" + 
+              (f" with score {score}" if score is not None else ""))
+        
+        return episode, score
 
-    def test(self, video_folder: str):
+    def test(self, video_folder=None, checkpoint_path=None, num_episodes=1):
         """Test the agent."""
         self.is_test = True
+        
+        if checkpoint_path:
+            self.load_checkpoint(checkpoint_path)
+        
+        if video_folder:
+            # 如果沒有提供視頻文件夾，創建一個
+            if not os.path.exists(video_folder):
+                os.makedirs(video_folder, exist_ok=True)
+                
+            tmp_env = self.env
+            self.env = gym.wrappers.RecordVideo(
+                self.env, 
+                video_folder=video_folder,
+                episode_trigger=lambda x: True  # 錄制所有episode
+            )
+        
+        scores = []
+        for ep in range(num_episodes):
+            state, _ = self.env.reset(seed=self.seed + ep)  # 使用不同的種子
+            done = False
+            score = 0
+            steps = 0
 
-        tmp_env = self.env
-        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder)
-
-        state, _ = self.env.reset(seed=self.seed)
-        done = False
-        score = 0
-
-        while not done:
-            action = self.select_action(state)
-            next_state, reward, done = self.step(action)
-
-            state = next_state
-            score += reward
+            while not done:
+                action = self.select_action(state)
+                next_state, reward, done = self.step(action)
 
-        print("score: ", score)
-        self.env.close()
+                state = next_state
+                score += reward
+                steps += 1
 
-        self.env = tmp_env
+            scores.append(score)
+            print(f"Test Episode {ep+1}/{num_episodes}: Score = {score}, Steps = {steps}")
+        
+        # 保存結果到CSV
+        if video_folder:
+            result_file = os.path.join(video_folder, 'test_results.csv')
+            with open(result_file, 'w', newline='') as f:
+                writer = csv.writer(f)
+                writer.writerow(['Episode', 'Score'])
+                for i, score in enumerate(scores):
+                    writer.writerow([i+1, score])
+                    
+            # 保存統計摘要
+            summary_file = os.path.join(video_folder, 'test_summary.json')
+            summary = {
+                'mean_score': float(np.mean(scores)),
+                'std_score': float(np.std(scores)),
+                'min_score': float(np.min(scores)),
+                'max_score': float(np.max(scores)),
+                'median_score': float(np.median(scores)),
+                'num_episodes': num_episodes
+            }
+            with open(summary_file, 'w') as f:
+                json.dump(summary, f, indent=4)
+            
+            print(f"Test Summary: Mean Score = {summary['mean_score']:.2f} ± {summary['std_score']:.2f}")
+            
+            # 關閉記錄環境
+            self.env.close()
+            self.env = tmp_env
+        
+        return scores
+
+    def save_config(self, args):
+        """保存配置到 YAML 文件"""
+        if self.is_test_mode:
+            return  # 在測試模式下跳過保存配置
+            
+        config = {
+            'actor_lr': self.actor_lr,
+            'critic_lr': self.critic_lr,
+            'discount_factor': self.gamma,
+            'entropy_weight': self.entropy_weight,
+            'seed': self.seed,
+            'num_episodes': self.num_episodes,
+            'actor_hidden_dim1': self.actor_hidden_dim1,
+            'actor_hidden_dim2': self.actor_hidden_dim2,
+            'critic_hidden_dim1': self.critic_hidden_dim1,
+            'critic_hidden_dim2': self.critic_hidden_dim2,
+            'save_per_epoch': self.save_per_epoch,
+        }
+        
+        with open(os.path.join(self.exp_dir, 'config.yaml'), 'w') as f:
+            yaml.dump(config, f, default_flow_style=False)
+            
+    def save_checkpoint(self, episode):
+        """保存模型檢查點"""
+        if self.is_test_mode:
+            return  # 在測試模式下跳過保存檢查點
+            
+        checkpoint = {
+            'actor_state_dict': self.actor.state_dict(),
+            'critic_state_dict': self.critic.state_dict(),
+            'actor_optimizer': self.actor_optimizer.state_dict(),
+            'critic_optimizer': self.critic_optimizer.state_dict(),
+            'episode': episode,
+        }
+        checkpoint_path = os.path.join(self.exp_dir, f'checkpoint_ep{episode}.pt')
+        torch.save(checkpoint, checkpoint_path)
+        print(f"Saved checkpoint at episode {episode} to {checkpoint_path}")
 
 def seed_torch(seed):
     torch.manual_seed(seed)
@@ -306,7 +471,18 @@ if __name__ == "__main__":
     parser.add_argument("--discount-factor", type=float, default=0.9)
     parser.add_argument("--num-episodes", type=float, default=5000)
     parser.add_argument("--seed", type=int, default=77)
-    parser.add_argument("--entropy-weight", type=int, default=1e-2) # entropy can be disabled by setting this to 0
+    parser.add_argument("--entropy-weight", type=float, default=1e-2) # entropy can be disabled by setting this to 0
+    
+    # 保存相關參數
+    parser.add_argument("--save-per-epoch", type=int, default=100, help="保存頻率（每多少個回合保存一次）")
+    parser.add_argument("--result-dir", type=str, default="result-a2c_pendulum", help="結果保存的基礎目錄")
+    
+    # 網絡相關參數
+    parser.add_argument("--actor-hidden-dim1", type=int, default=128)
+    parser.add_argument("--actor-hidden-dim2", type=int, default=64)
+    parser.add_argument("--critic-hidden-dim1", type=int, default=128)
+    parser.add_argument("--critic-hidden-dim2", type=int, default=64)
+    
     args = parser.parse_args()
     
     # environment
diff --git a/sweep_a2c.py b/sweep_a2c.py
index e3d38d2..899b705 100644
--- a/sweep_a2c.py
+++ b/sweep_a2c.py
@@ -42,6 +42,9 @@ def train_agent_with_config():
             self.actor_hidden_dim2 = config.actor_hidden_dim2
             self.critic_hidden_dim1 = config.critic_hidden_dim1
             self.critic_hidden_dim2 = config.critic_hidden_dim2
+            # 保存相關參數
+            self.save_per_epoch = getattr(config, 'save_per_epoch', 100)
+            self.result_dir = "result-a2c_pendulum_sweep"
     
     args = Args(config)
     
@@ -95,6 +98,9 @@ sweep_config = {
         'num_episodes': {
             'value': 500  # 固定評估回合數以節省時間
         },
+        'save_per_epoch': {
+            'value': 100  # 每 100 個回合保存一次檢查點
+        },
         'seed': {
             'values': [42, 77, 123, 456, 789]  # 多個隨機種子以提高穩定性
         }
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 180c501..c38f685 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250514_131510-pacfausg/logs/debug-internal.log
\ No newline at end of file
+run-20250514_133924-tn1pz4zi/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 923081d..94bca5f 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250514_131510-pacfausg/logs/debug.log
\ No newline at end of file
+run-20250514_133924-tn1pz4zi/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index f9fa412..13468bd 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250514_131510-pacfausg
\ No newline at end of file
+run-20250514_133924-tn1pz4zi
\ No newline at end of file
diff --git a/wandb/run-20250514_131510-pacfausg/files/output.log b/wandb/run-20250514_131510-pacfausg/files/output.log
index 7dd2bf6..4cbeb4a 100644
--- a/wandb/run-20250514_131510-pacfausg/files/output.log
+++ b/wandb/run-20250514_131510-pacfausg/files/output.log
@@ -270,3 +270,232 @@ Episode 267: Total Reward = -1001.2958298544174
 Episode 268: Total Reward = -1179.0747309417843
 Episode 269: Total Reward = -1146.7207832675438
 Episode 270: Total Reward = -1216.9410690143864
+Episode 271: Total Reward = -1125.0928486554715
+Episode 272: Total Reward = -1134.899899398173
+Episode 273: Total Reward = -1139.4934575888053
+Episode 274: Total Reward = -1166.147840819611
+Episode 275: Total Reward = -1045.4546655224033
+Episode 276: Total Reward = -1185.2257356077644
+Episode 277: Total Reward = -1037.7375950619492
+Episode 278: Total Reward = -1152.1653142353166
+Episode 279: Total Reward = -988.3037279928701
+Episode 280: Total Reward = -1055.9835563409122
+Episode 281: Total Reward = -1159.2873744421597
+Episode 282: Total Reward = -1066.4442438721785
+Episode 283: Total Reward = -1109.737052221302
+Episode 284: Total Reward = -1041.7051272559377
+Episode 285: Total Reward = -1142.787317598404
+Episode 286: Total Reward = -1207.3645316013453
+Episode 287: Total Reward = -1032.973192763897
+Episode 288: Total Reward = -1168.445196715978
+Episode 289: Total Reward = -1139.5593124405948
+Episode 290: Total Reward = -1099.644003731351
+Episode 291: Total Reward = -1167.1604446176018
+Episode 292: Total Reward = -1039.815300719862
+Episode 293: Total Reward = -1065.164684403906
+Episode 294: Total Reward = -1127.2736568819869
+Episode 295: Total Reward = -1115.6666312777181
+Episode 296: Total Reward = -1147.0563688530988
+Episode 297: Total Reward = -1143.7271821560837
+Episode 298: Total Reward = -1158.5329495962924
+Episode 299: Total Reward = -1115.9303905559543
+Episode 300: Total Reward = -1079.3057727624096
+Episode 301: Total Reward = -1024.2181290690576
+Episode 302: Total Reward = -1021.0298372522057
+Episode 303: Total Reward = -1178.699614835674
+Episode 304: Total Reward = -1153.9570852544105
+Episode 305: Total Reward = -1017.8462376771912
+Episode 306: Total Reward = -1142.7709740706664
+Episode 307: Total Reward = -1114.91424439293
+Episode 308: Total Reward = -1160.2540385653324
+Episode 309: Total Reward = -1147.6666596330658
+Episode 310: Total Reward = -1140.4204553685556
+Episode 311: Total Reward = -1195.6304532739337
+Episode 312: Total Reward = -1170.408428167271
+Episode 313: Total Reward = -1026.2013280784122
+Episode 314: Total Reward = -1053.376685671363
+Episode 315: Total Reward = -1151.2967087062027
+Episode 316: Total Reward = -1106.6720724550476
+Episode 317: Total Reward = -1066.8356794438896
+Episode 318: Total Reward = -1134.2551174043192
+Episode 319: Total Reward = -1023.1360890521075
+Episode 320: Total Reward = -1032.7224154268026
+Episode 321: Total Reward = -1160.955531990823
+Episode 322: Total Reward = -1185.5442813891764
+Episode 323: Total Reward = -1077.561608980812
+Episode 324: Total Reward = -1043.28473650608
+Episode 325: Total Reward = -1144.3357666256074
+Episode 326: Total Reward = -1108.546960379675
+Episode 327: Total Reward = -1146.9965232933678
+Episode 328: Total Reward = -1153.5831348949052
+Episode 329: Total Reward = -1151.7268636112954
+Episode 330: Total Reward = -1136.9522040602521
+Episode 331: Total Reward = -1142.3815901811556
+Episode 332: Total Reward = -1109.0490419285527
+Episode 333: Total Reward = -997.9243206406651
+Episode 334: Total Reward = -1022.8334175784072
+Episode 335: Total Reward = -1004.9857219294997
+Episode 336: Total Reward = -1121.2162601229973
+Episode 337: Total Reward = -1015.9388116819238
+Episode 338: Total Reward = -1030.3609859165933
+Episode 339: Total Reward = -1029.5786128636516
+Episode 340: Total Reward = -1113.3186429002767
+Episode 341: Total Reward = -1127.6552882093777
+Episode 342: Total Reward = -1106.8843267620184
+Episode 343: Total Reward = -1136.2874193655
+Episode 344: Total Reward = -991.5907707704677
+Episode 345: Total Reward = -755.1086913029689
+Episode 346: Total Reward = -1115.0413224752967
+Episode 347: Total Reward = -1082.6913230029377
+Episode 348: Total Reward = -1133.4183448175913
+Episode 349: Total Reward = -1028.649324757055
+Episode 350: Total Reward = -1097.2373172801217
+Episode 351: Total Reward = -1008.1217631148168
+Episode 352: Total Reward = -1142.447921475509
+Episode 353: Total Reward = -1104.4346578702275
+Episode 354: Total Reward = -1145.504767579076
+Episode 355: Total Reward = -1045.9367107417452
+Episode 356: Total Reward = -1095.4723199664825
+Episode 357: Total Reward = -1017.296775897898
+Episode 358: Total Reward = -1112.3199513703314
+Episode 359: Total Reward = -1127.8964661789237
+Episode 360: Total Reward = -1099.852842876761
+Episode 361: Total Reward = -1158.7965887325095
+Episode 362: Total Reward = -1136.3830066265518
+Episode 363: Total Reward = -1147.484381590022
+Episode 364: Total Reward = -1171.8259552137836
+Episode 365: Total Reward = -1111.6000619960907
+Episode 366: Total Reward = -1110.1137260318515
+Episode 367: Total Reward = -1143.1180687766312
+Episode 368: Total Reward = -1033.1630251657627
+Episode 369: Total Reward = -1176.6769676418783
+Episode 370: Total Reward = -1127.3644476827253
+Episode 371: Total Reward = -1031.2899538803586
+Episode 372: Total Reward = -1077.5534015389455
+Episode 373: Total Reward = -1004.7556899919144
+Episode 374: Total Reward = -1125.0584399464803
+Episode 375: Total Reward = -1020.7342030053368
+Episode 376: Total Reward = -1029.6323709071282
+Episode 377: Total Reward = -1002.6259083040319
+Episode 378: Total Reward = -1141.7835249570196
+Episode 379: Total Reward = -1158.2751692704553
+Episode 380: Total Reward = -1149.1480947460368
+Episode 381: Total Reward = -1148.964883947839
+Episode 382: Total Reward = -1087.1098648182367
+Episode 383: Total Reward = -1144.9200439042784
+Episode 384: Total Reward = -1154.7507305904307
+Episode 385: Total Reward = -1139.0197760640428
+Episode 386: Total Reward = -1147.3143996112276
+Episode 387: Total Reward = -1146.150611078135
+Episode 388: Total Reward = -1141.6665107780443
+Episode 389: Total Reward = -1067.7516443149868
+Episode 390: Total Reward = -1103.9326007045495
+Episode 391: Total Reward = -1140.973055106239
+Episode 392: Total Reward = -1093.8918956264502
+Episode 393: Total Reward = -1008.4398403329922
+Episode 394: Total Reward = -1141.2880835497292
+Episode 395: Total Reward = -1003.4162580035826
+Episode 396: Total Reward = -1046.1374799894365
+Episode 397: Total Reward = -1166.282516379359
+Episode 398: Total Reward = -1171.4220351543393
+Episode 399: Total Reward = -947.6709434714439
+Episode 400: Total Reward = -1011.069840400849
+Episode 401: Total Reward = -985.5945394636828
+Episode 402: Total Reward = -1183.317549378793
+Episode 403: Total Reward = -1018.834457260337
+Episode 404: Total Reward = -1127.4589580370778
+Episode 405: Total Reward = -1150.183136501994
+Episode 406: Total Reward = -1153.740341616593
+Episode 407: Total Reward = -1194.4769025188332
+Episode 408: Total Reward = -1104.6190591342265
+Episode 409: Total Reward = -1027.9608704558539
+Episode 410: Total Reward = -1127.4441940504612
+Episode 411: Total Reward = -1049.1588533380227
+Episode 412: Total Reward = -1146.7727330380014
+Episode 413: Total Reward = -998.1613101246121
+Episode 414: Total Reward = -1135.565886013904
+Episode 415: Total Reward = -1018.731695025773
+Episode 416: Total Reward = -1140.5702031509172
+Episode 417: Total Reward = -1115.1524247898785
+Episode 418: Total Reward = -1132.0449144711854
+Episode 419: Total Reward = -1107.6773591149201
+Episode 420: Total Reward = -1011.4883206322829
+Episode 421: Total Reward = -996.0244998067265
+Episode 422: Total Reward = -1125.8180092510067
+Episode 423: Total Reward = -1067.1389211700402
+Episode 424: Total Reward = -1098.551259249835
+Episode 425: Total Reward = -1055.189413578483
+Episode 426: Total Reward = -1166.2223207026034
+Episode 427: Total Reward = -1110.9471040686547
+Episode 428: Total Reward = -1114.4201032501549
+Episode 429: Total Reward = -1134.6637695922682
+Episode 430: Total Reward = -1094.1112235318606
+Episode 431: Total Reward = -1143.017127108731
+Episode 432: Total Reward = -1099.635760133173
+Episode 433: Total Reward = -1166.2737190702708
+Episode 434: Total Reward = -1145.0784178165911
+Episode 435: Total Reward = -1018.1756762817911
+Episode 436: Total Reward = -1133.1948594578566
+Episode 437: Total Reward = -1115.3495714495446
+Episode 438: Total Reward = -1086.2621040554095
+Episode 439: Total Reward = -1002.061876243505
+Episode 440: Total Reward = -1042.094544725928
+Episode 441: Total Reward = -1095.320140445414
+Episode 442: Total Reward = -1106.589241598921
+Episode 443: Total Reward = -1121.6534772789062
+Episode 444: Total Reward = -1118.626293052231
+Episode 445: Total Reward = -993.2549909968848
+Episode 446: Total Reward = -1133.6157233966521
+Episode 447: Total Reward = -1175.3852622751995
+Episode 448: Total Reward = -1010.304244097848
+Episode 449: Total Reward = -1101.744484687596
+Episode 450: Total Reward = -1128.3279639033979
+Episode 451: Total Reward = -1111.8023157050566
+Episode 452: Total Reward = -1120.8671122198814
+Episode 453: Total Reward = -1012.5451498264729
+Episode 454: Total Reward = -1099.4457021957492
+Episode 455: Total Reward = -1054.254158031382
+Episode 456: Total Reward = -1100.4370469327173
+Episode 457: Total Reward = -1074.2630076824478
+Episode 458: Total Reward = -1129.6531164629625
+Episode 459: Total Reward = -1090.3287126850298
+Episode 460: Total Reward = -1027.6682066154597
+Episode 461: Total Reward = -975.7790672142985
+Episode 462: Total Reward = -1044.4386576516831
+Episode 463: Total Reward = -1097.8361037445286
+Episode 464: Total Reward = -1007.1580312227787
+Episode 465: Total Reward = -1125.7377946205434
+Episode 466: Total Reward = -1121.6403499153448
+Episode 467: Total Reward = -991.0913376800916
+Episode 468: Total Reward = -1138.5474930362493
+Episode 469: Total Reward = -1009.5245083149653
+Episode 470: Total Reward = -861.4896140561519
+Episode 471: Total Reward = -1049.6269693971817
+Episode 472: Total Reward = -1009.2103668169126
+Episode 473: Total Reward = -1041.9952597716265
+Episode 474: Total Reward = -1066.6410932835315
+Episode 475: Total Reward = -1035.8536538737292
+Episode 476: Total Reward = -1089.426376093483
+Episode 477: Total Reward = -1019.4031404262505
+Episode 478: Total Reward = -1002.2438758736758
+Episode 479: Total Reward = -997.3287593235519
+Episode 480: Total Reward = -900.8593684707935
+Episode 481: Total Reward = -995.5005220962402
+Episode 482: Total Reward = -1014.7567021232477
+Episode 483: Total Reward = -1088.5129103052268
+Episode 484: Total Reward = -1144.3033150241583
+Episode 485: Total Reward = -1158.8319988882347
+Episode 486: Total Reward = -1016.4038457082185
+Episode 487: Total Reward = -1120.3784371230722
+Episode 488: Total Reward = -1092.303654030679
+Episode 489: Total Reward = -1081.284706764015
+Episode 490: Total Reward = -1038.0274437079177
+Episode 491: Total Reward = -1152.1854125986429
+Episode 492: Total Reward = -1136.074486899722
+Episode 493: Total Reward = -1035.452237300649
+Episode 494: Total Reward = -1051.4087179980293
+Episode 495: Total Reward = -979.2622464257005
+Episode 496: Total Reward = -1085.8863687384828
+Episode 497: Total Reward = -1051.0796486244287
+Episode 498: Total Reward = -1074.4104822461054
+Episode 499: Total Reward = -1046.3903504095033
diff --git a/wandb/run-20250514_131510-pacfausg/logs/debug-internal.log b/wandb/run-20250514_131510-pacfausg/logs/debug-internal.log
index 7871b68..67b851b 100644
--- a/wandb/run-20250514_131510-pacfausg/logs/debug-internal.log
+++ b/wandb/run-20250514_131510-pacfausg/logs/debug-internal.log
@@ -6,3 +6,12 @@
 {"time":"2025-05-14T13:15:11.191788078Z","level":"INFO","msg":"handler: started","stream_id":"pacfausg"}
 {"time":"2025-05-14T13:15:11.430611214Z","level":"INFO","msg":"Starting system monitor"}
 {"time":"2025-05-14T13:15:11.430756339Z","level":"ERROR","msg":"git repo not found","error":"repository does not exist"}
+{"time":"2025-05-14T13:18:32.437998199Z","level":"INFO","msg":"Stopping system monitor"}
+{"time":"2025-05-14T13:18:32.43803927Z","level":"INFO","msg":"Stopped system monitor"}
+{"time":"2025-05-14T13:18:33.389476121Z","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
+{"time":"2025-05-14T13:18:33.440330373Z","level":"INFO","msg":"handler: operation stats","stats":{"operations":[{"desc":"uploading history steps 97516-100298, summary, console lines 487-500","runtime_seconds":0.043829649}],"total_operations":1}}
+{"time":"2025-05-14T13:18:33.724119486Z","level":"INFO","msg":"stream: closing","id":"pacfausg"}
+{"time":"2025-05-14T13:18:33.724150088Z","level":"INFO","msg":"handler: closed","stream_id":"pacfausg"}
+{"time":"2025-05-14T13:18:33.724165114Z","level":"INFO","msg":"writer: Close: closed","stream_id":"pacfausg"}
+{"time":"2025-05-14T13:18:33.724254293Z","level":"INFO","msg":"sender: closed","stream_id":"pacfausg"}
+{"time":"2025-05-14T13:18:33.724323258Z","level":"INFO","msg":"stream: closed","id":"pacfausg"}
diff --git a/wandb/run-20250514_131510-pacfausg/logs/debug.log b/wandb/run-20250514_131510-pacfausg/logs/debug.log
index 4067940..59e1960 100644
--- a/wandb/run-20250514_131510-pacfausg/logs/debug.log
+++ b/wandb/run-20250514_131510-pacfausg/logs/debug.log
@@ -21,3 +21,10 @@ config: {'_wandb': {'code_path': 'code/sweep_a2c.py'}}
 2025-05-14 13:15:11,457 INFO    Thread-3 (_run_job):1480989 [wandb_run.py:_redirect():2490] Wrapping output streams.
 2025-05-14 13:15:11,457 INFO    Thread-3 (_run_job):1480989 [wandb_run.py:_redirect():2513] Redirects installed.
 2025-05-14 13:15:11,457 INFO    Thread-3 (_run_job):1480989 [wandb_init.py:init():1150] run started, returning control to user process
+2025-05-14 13:18:32,437 INFO    Thread-3 (_run_job):1480989 [wandb_run.py:_finish():2321] finishing run rainsoon717/DLP-Lab7-A2C-Pendulum-Sweep/pacfausg
+2025-05-14 13:18:32,437 INFO    Thread-3 (_run_job):1480989 [wandb_run.py:_atexit_cleanup():2538] got exitcode: 0
+2025-05-14 13:18:32,437 INFO    Thread-3 (_run_job):1480989 [wandb_run.py:_restore():2520] restore
+2025-05-14 13:18:32,437 INFO    Thread-3 (_run_job):1480989 [wandb_run.py:_restore():2526] restore done
+2025-05-14 13:18:33,721 INFO    Thread-3 (_run_job):1480989 [wandb_run.py:_footer_history_summary_info():4188] rendering history
+2025-05-14 13:18:33,722 INFO    Thread-3 (_run_job):1480989 [wandb_run.py:_footer_history_summary_info():4220] rendering summary
+2025-05-14 13:18:33,723 INFO    Thread-3 (_run_job):1480989 [wandb_run.py:_footer_sync_info():4149] logging synced files
diff --git a/wandb/run-20250514_131510-pacfausg/run-pacfausg.wandb b/wandb/run-20250514_131510-pacfausg/run-pacfausg.wandb
index b76dcbe..48b6ef6 100644
Binary files a/wandb/run-20250514_131510-pacfausg/run-pacfausg.wandb and b/wandb/run-20250514_131510-pacfausg/run-pacfausg.wandb differ
