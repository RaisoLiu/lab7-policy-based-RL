{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, input): return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert numpy arrays to tensors\n",
    "def t(x):\n",
    "    x = np.array(x) if not isinstance(x, np.ndarray) else x\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "        \n",
    "        logstds_param = nn.Parameter(torch.full((n_actions,), 0.1))\n",
    "        self.register_parameter(\"logstds\", logstds_param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        means = self.model(X)\n",
    "        stds = torch.clamp(self.logstds.exp(), 1e-3, 50)\n",
    "        \n",
    "        return torch.distributions.Normal(means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Critic module\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(rewards, dones, gamma):\n",
    "    ret = 0\n",
    "    discounted = []\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        ret = reward + ret * gamma * (1-done)\n",
    "        discounted.append(ret)\n",
    "    \n",
    "    return discounted[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_memory(memory, gamma=0.99, discount_rewards=True):\n",
    "    actions = []\n",
    "    states = []\n",
    "    next_states = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "\n",
    "    for action, reward, state, next_state, done in memory:\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "    \n",
    "    if discount_rewards:\n",
    "        if False and dones[-1] == 0:\n",
    "            rewards = discounted_rewards(rewards + [last_value], dones + [0], gamma)[:-1]\n",
    "        else:\n",
    "            rewards = discounted_rewards(rewards, dones, gamma)\n",
    "\n",
    "    actions = t(actions).view(-1, 1)\n",
    "    states = t(states)\n",
    "    next_states = t(next_states)\n",
    "    rewards = t(rewards).view(-1, 1)\n",
    "    dones = t(dones).view(-1, 1)\n",
    "    return actions, rewards, states, next_states, dones\n",
    "\n",
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CLearner():\n",
    "    def __init__(self, actor, critic, gamma=0.9, entropy_beta=0,\n",
    "                 actor_lr=4e-4, critic_lr=4e-3, max_grad_norm=0.5):\n",
    "        self.gamma = gamma\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.actor_optim = torch.optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optim = torch.optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def learn(self, memory, steps, discount_rewards=True):\n",
    "        actions, rewards, states, next_states, dones = process_memory(memory, self.gamma, discount_rewards)\n",
    "\n",
    "        if discount_rewards:\n",
    "            td_target = rewards\n",
    "        else:\n",
    "            td_target = rewards + self.gamma*critic(next_states)*(1-dones)\n",
    "        value = critic(states)\n",
    "        advantage = td_target - value\n",
    "\n",
    "        # actor\n",
    "        norm_dists = self.actor(states)\n",
    "        logs_probs = norm_dists.log_prob(actions)\n",
    "        entropy = norm_dists.entropy().mean()\n",
    "        \n",
    "        actor_loss = (-logs_probs*advantage.detach()).mean() - entropy*self.entropy_beta\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        \n",
    "        clip_grad_norm_(self.actor_optim, self.max_grad_norm)\n",
    "        writer.add_histogram(\"gradients/actor\",\n",
    "                             torch.cat([p.grad.view(-1) for p in self.actor.parameters()]), global_step=steps)\n",
    "        writer.add_histogram(\"parameters/actor\",\n",
    "                             torch.cat([p.data.view(-1) for p in self.actor.parameters()]), global_step=steps)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # critic\n",
    "        critic_loss = F.mse_loss(td_target, value)\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        clip_grad_norm_(self.critic_optim, self.max_grad_norm)\n",
    "        writer.add_histogram(\"gradients/critic\",\n",
    "                             torch.cat([p.grad.view(-1) for p in self.critic.parameters()]), global_step=steps)\n",
    "        writer.add_histogram(\"parameters/critic\",\n",
    "                             torch.cat([p.data.view(-1) for p in self.critic.parameters()]), global_step=steps)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # reports\n",
    "        writer.add_scalar(\"losses/log_probs\", -logs_probs.mean(), global_step=steps)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy, global_step=steps) \n",
    "        writer.add_scalar(\"losses/entropy_beta\", self.entropy_beta, global_step=steps) \n",
    "        writer.add_scalar(\"losses/actor\", actor_loss, global_step=steps)\n",
    "        writer.add_scalar(\"losses/advantage\", advantage.mean(), global_step=steps)\n",
    "        writer.add_scalar(\"losses/critic\", critic_loss, global_step=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state = None\n",
    "        self.done = True\n",
    "        self.steps = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.episode_reward = 0\n",
    "        self.done = False\n",
    "        self.state, _ = self.env.reset()\n",
    "    \n",
    "    def run(self, max_steps, memory=None):\n",
    "        if not memory: memory = []\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            if self.done: \n",
    "                self.reset()\n",
    "                \n",
    "            # print(\"self.state:\", self.state)\n",
    "            \n",
    "            dists = actor(t(self.state).to(device))\n",
    "            print(\"location of dists:\", dists.device)\n",
    "\n",
    "            # print(\"dists:\", dists)\n",
    "            \n",
    "            actions = dists.sample().detach()\n",
    "            actions_clipped = np.clip(actions, self.env.action_space.low.min(), env.action_space.high.max())\n",
    "            output = self.env.step(actions_clipped)\n",
    "            # print(\"output:\", output)\n",
    "            next_state, reward, self.done = output[:3]\n",
    "            memory.append((actions, reward, self.state, next_state, self.done))\n",
    "\n",
    "            self.state = next_state\n",
    "            self.steps += 1\n",
    "            self.episode_reward += reward\n",
    "            \n",
    "            if self.done:\n",
    "                self.episode_rewards.append(self.episode_reward)\n",
    "                if len(self.episode_rewards) % 10 == 0:\n",
    "                    print(\"episode:\", len(self.episode_rewards), \", episode reward:\", self.episode_reward)\n",
    "                writer.add_scalar(\"episode_reward\", self.episode_reward, global_step=self.steps)\n",
    "                    \n",
    "        \n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "writer = SummaryWriter(\"runs/mish_activation\")\n",
    "\n",
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "actor = Actor(state_dim, n_actions, activation=Mish).to(device)\n",
    "critic = Critic(state_dim, activation=Mish).to(device)\n",
    "\n",
    "learner = A2CLearner(actor, critic)\n",
    "runner = Runner(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Normal' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m (episode_length\u001b[38;5;241m*\u001b[39mepisodes)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39msteps_on_memory\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_steps):\n\u001b[0;32m----> 7\u001b[0m     memory \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps_on_memory\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     learner\u001b[38;5;241m.\u001b[39mlearn(memory, runner\u001b[38;5;241m.\u001b[39msteps, discount_rewards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self, max_steps, memory)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(\"self.state:\", self.state)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m dists \u001b[38;5;241m=\u001b[39m actor(t(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation of dists:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdists\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(\"dists:\", dists)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m actions \u001b[38;5;241m=\u001b[39m dists\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mdetach()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Normal' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "steps_on_memory = 16\n",
    "episodes = 500\n",
    "episode_length = 200\n",
    "total_steps = (episode_length*episodes)//steps_on_memory\n",
    "\n",
    "for i in range(total_steps):\n",
    "    memory = runner.run(steps_on_memory).to(device)\n",
    "    learner.learn(memory, runner.steps, discount_rewards=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
